{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0277c592-f033-4e60-b477-4e1e4ef9e9ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Read CSV with header\n",
    "df = spark.read.format('csv').option('header', 'true').load('Files/orders/2019.csv')\n",
    "display(df)\n",
    "# Step 2: Read CSV without header\n",
    "df = spark.read.format('csv').option('header', 'false').load('Files/orders/2019.csv')\n",
    "display(df)\n",
    "# Step 3: Define schema and read CSV with schema\n",
    "from pyspark.sql.types import *\n",
    "orderSchema = StructType([\n",
    "    StructField('SalesOrderNumber', StringType()),\n",
    "    StructField('SalesOrderLineNumber', IntegerType()),\n",
    "    StructField('OrderDate', DateType()),\n",
    "    StructField('CustomerName', StringType()),\n",
    "    StructField('Email', StringType()),\n",
    "    StructField('Item', StringType()),\n",
    "    StructField('Quantity', IntegerType()),\n",
    "    StructField('UnitPrice', FloatType()),\n",
    "    StructField('Tax', FloatType())\n",
    "])\n",
    "df = spark.read.format('csv').schema(orderSchema).load('Files/orders/2019.csv')\n",
    "display(df)\n",
    "# Step 4: Load multiple CSV files\n",
    "df = spark.read.format('csv').schema(orderSchema).load('Files/orders/*.csv')\n",
    "display(df)\n",
    "# Step 5: Get distinct customers\n",
    "customers = df.select('CustomerName', 'Email')\n",
    "print(customers.count())\n",
    "print(customers.distinct().count())\n",
    "display(customers.distinct())\n",
    "# Step 6: Filter customers who ordered a specific item\n",
    "customers = df.select('CustomerName', 'Email').where(df['Item'] == 'Road-250 Red, 52')\n",
    "print(customers.count())\n",
    "print(customers.distinct().count())\n",
    "display(customers.distinct())\n",
    "# Step 7: Group sales by item\n",
    "productSales = df.select('Item', 'Quantity').groupBy('Item').sum()\n",
    "display(productSales)\n",
    "# Step 8: Count yearly sales\n",
    "from pyspark.sql.functions import *\n",
    "yearlySales = df.select(year(col('OrderDate')).alias('Year')).groupBy('Year').count().orderBy('Year')\n",
    "display(yearlySales)\n",
    "# Step 9: Transform data â€“ add Year, Month, FirstName, LastName\n",
    "from pyspark.sql.functions import *\n",
    "transformed_df = df.withColumn('Year', year(col('OrderDate'))).withColumn('Month', month(col('OrderDate')))\n",
    "transformed_df = transformed_df.withColumn('FirstName', split(col('CustomerName'), ' ').getItem(0)).withColumn('LastName', split(col('CustomerName'), ' ').getItem(1))\n",
    "transformed_df = transformed_df.select('SalesOrderNumber','SalesOrderLineNumber','OrderDate','Year','Month','FirstName','LastName','Email','Item','Quantity','UnitPrice','Tax')\n",
    "display(transformed_df.limit(5))\n",
    "# Step 10: Save transformed data\n",
    "transformed_df.write.mode('overwrite').parquet('Files/transformed_data/orders')\n",
    "print('Transformed data saved!')\n",
    "# Step 11: Read Parquet data\n",
    "orders_df = spark.read.format('parquet').load('Files/transformed_data/orders')\n",
    "display(orders_df)\n",
    "# Step 12: Partition Parquet data by Year and Month\n",
    "orders_df.write.partitionBy('Year','Month').mode('overwrite').parquet('Files/partitioned_data')\n",
    "print('Transformed data saved!')\n",
    "# Step 13: Read specific partition\n",
    "orders_2021_df = spark.read.format('parquet').load('Files/partitioned_data/Year=2021/Month=*')\n",
    "display(orders_2021_df)\n",
    "# Step 14: Save as Delta table\n",
    "df.write.format('delta').saveAsTable('salesorders')\n",
    "spark.sql('DESCRIBE EXTENDED salesorders').show(truncate=False)\n",
    "# Step 15: Query table from catalog\n",
    "df = spark.sql('SELECT * FROM practicels.salesorders LIMIT 1000')\n",
    "display(df)\n",
    "# Step 16: SQL Query for yearly revenue\n",
    "%%sql\n",
    "SELECT YEAR(OrderDate) AS OrderYear, SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue FROM salesorders GROUP BY YEAR(OrderDate) ORDER BY OrderYear;\n",
    "# Step 17: Compute yearly metrics (PySpark SQL)\n",
    "sqlQuery = '''\n",
    "SELECT CAST(YEAR(OrderDate) AS CHAR(4)) AS OrderYear, SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue, COUNT(DISTINCT SalesOrderNumber) AS YearlyCounts FROM salesorders GROUP BY CAST(YEAR(OrderDate) AS CHAR(4)) ORDER BY OrderYear'''\n",
    "df_spark = spark.sql(sqlQuery)\n",
    "df_spark.show()\n",
    "# Step 18: Convert to Pandas and visualize revenue\n",
    "from matplotlib import pyplot as plt\n",
    "df_sales = df_spark.toPandas()\n",
    "plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'])\n",
    "plt.show()\n",
    "# Step 19: Seaborn Visualization\n",
    "import seaborn as sns\n",
    "plt.clf()\n",
    "sns.set_theme(style='whitegrid')\n",
    "sns.barplot(x='OrderYear', y='GrossRevenue', data=df_sales)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06edc74e-8c5d-4ca5-9fa4-8fefb0876aca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lab1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
